{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BioInformatics Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eove2UF4ETz0",
        "vJGInzIKWO7_",
        "EaEx6RDaWRqE",
        "K-9ha9_YEVZx",
        "qf0MWApibhLg",
        "nuFMFKX7EXZH",
        "cbN-ijuNc1L_",
        "qte0fXVLE_nY",
        "tBpNldSDkCJ-",
        "1VJPTlwpc_0R",
        "JN69EDKVNebQ",
        "fXCpyceok0z7",
        "eO5ib5__Nzt8",
        "reZOGfDoeVmA",
        "QqvClMbueavg",
        "V6ullEZfa91-",
        "egstmi55FCwl"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eove2UF4ETz0"
      },
      "source": [
        "# Presets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJGInzIKWO7_"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byoaMy3oEmd9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "import PIL as pil\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from IPython import display\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "gan1, gan2 = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaEx6RDaWRqE"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnQ2933pEUmi"
      },
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.discriminator_optimizer = d_optimizer\n",
        "        self.generator_optimizer = g_optimizer\n",
        "\n",
        "    def train_step(self, images):\n",
        "        noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "          generated_images = self.generator(noise, training=True)\n",
        "\n",
        "          real_output = self.discriminator(images, training=True)\n",
        "          fake_output = self.discriminator(generated_images, training=True)\n",
        "\n",
        "          gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "          real_loss = self.loss_fn(tf.ones_like(real_output), real_output)\n",
        "          fake_loss = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "          disc_loss = real_loss + fake_loss\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "        return {\"d_loss\": disc_loss, \"g_loss\": gen_loss}\n",
        "\n",
        "def restore_gan(ver):\n",
        "  if ver == 1:\n",
        "    early_stride = (1,1)\n",
        "    late_stride = (2,2)\n",
        "    ckpts_path = '/content/gdrive/My Drive/normal_gan_ckpts'\n",
        "    lr = 0.0001\n",
        "  elif ver == 2:\n",
        "    early_stride = (2,2)\n",
        "    late_stride = (1,1)\n",
        "    ckpts_path = '/content/gdrive/My Drive/deep_aug_dims_ckpts'\n",
        "    lr = 0.00001\n",
        "  else:\n",
        "    print(\"Wrong version index\")\n",
        "    return\n",
        "\n",
        "  discriminator = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.InputLayer((512, 512, 3)),\n",
        "\n",
        "          tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "          tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "      \n",
        "          tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "      \n",
        "          tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "      \n",
        "          tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same'),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(1),\n",
        "      ],\n",
        "      name=\"discriminator\",\n",
        "  )\n",
        "  latent_dim = 100\n",
        "  generator = tf.keras.Sequential(\n",
        "      [\n",
        "          tf.keras.layers.InputLayer(latent_dim),\n",
        "          tf.keras.layers.Dense(128 * 128 * 128, use_bias=False, input_shape=(100,)),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "          tf.keras.layers.Reshape((128, 128, 128)),\n",
        "          tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=early_stride, padding='same', use_bias=False),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "          tf.keras.layers.BatchNormalization(),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "          tf.keras.layers.Conv2DTranspose(3, (4, 4), strides=late_stride, padding='same', use_bias=False, activation='sigmoid'),\n",
        "      ],\n",
        "      name=\"generator\",\n",
        "  )\n",
        "  gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "  gan.compile(\n",
        "      d_optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "      g_optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "      loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "  )\n",
        "  checkpoint = tf.train.Checkpoint(gan)\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(ckpts_path))\n",
        "  return gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDzE1HjAWUvl"
      },
      "source": [
        "def create_extractor(discriminator):\n",
        "  input = tf.keras.layers.Input(shape=(512, 512, 3))\n",
        "  extractor = tf.keras.models.Model(discriminator.layers[0].input, discriminator.layers[-3].output)\n",
        "  extractor.trainable = False\n",
        "  x = extractor(input)\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  return tf.keras.models.Model(input, x)\n",
        "\n",
        "def extract_features(extractor, type='all'):\n",
        "  dir = \"/content/gdrive/MyDrive/dataset\"\n",
        "  ds = tfds.load(\"diabetic_retinopathy_detection/btgraham-300\", data_dir=dir, shuffle_files=True)\n",
        "  train = ds['train'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "  test = ds['test'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "\n",
        "  if type == 'all':\n",
        "    features = []\n",
        "    labels = []\n",
        "    for i, sample in enumerate(train):\n",
        "      features.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "      labels.append(sample[1])\n",
        "      print('\\rTraining Dataset to list: %.0f%%'%(((i+1)/len(train))*100), end='')\n",
        "    test_feats = []\n",
        "    test_labs = []\n",
        "    for i, sample in enumerate(test):\n",
        "      test_feats.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "      test_labs.append(sample[1])\n",
        "      print('\\rValidation Dataset to list: %.0f%%'%(((i+1)/len(test))*100), end='')\n",
        "    return np.array(features), np.array(labels), np.array(test_feats), np.array(test_labs)\n",
        "  elif type == 'train':\n",
        "    features = []\n",
        "    labels = []\n",
        "    for i, sample in enumerate(train):\n",
        "      features.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "      labels.append(sample[1])\n",
        "      print('\\rTraining Dataset to list: %.0f%%'%(((i+1)/len(train))*100), end='')\n",
        "    return np.array(features), np.array(labels)\n",
        "  else:\n",
        "    test_feats = []\n",
        "    test_labs = []\n",
        "    for i, sample in enumerate(test):\n",
        "      test_feats.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "      test_labs.append(sample[1])\n",
        "      print('\\rValidation Dataset to list: %.0f%%'%(((i+1)/len(test))*100), end='')\n",
        "    return np.array(test_feats), np.array(test_labs)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-9ha9_YEVZx"
      },
      "source": [
        "# Diabetic Retinopathy Detection Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfClUVCHmmNj"
      },
      "source": [
        "#from google.colab import files\n",
        "! pip install -q kaggle\n",
        "# files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/gdrive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d prasertsak/dr2015-resized\n",
        "!unzip -q dr2015-resized.zip -d dr2015-resized\n",
        "!rm dr2015-resized.zip\n",
        "\n",
        "!mkdir -p /content/dr2015-resized/manual\n",
        "\n",
        "!mv /content/dr2015-resized/resized_train/resized_train /content/dr2015-resized/manual/train\n",
        "!mv /content/dr2015-resized/resized_test/resized_test /content/dr2015-resized/manual/test\n",
        "!mv /content/dr2015-resized/sampleSubmission.csv /content/dr2015-resized/manual/\n",
        "!mv /content/dr2015-resized/trainLabels.csv /content/dr2015-resized/manual/\n",
        "\n",
        "!mkdir -p /content/dr2015-resized/manual/sample\n",
        "\n",
        "!cp /content/dr2015-resized/manual/train/10003_left.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10003_right.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10007_left.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10007_right.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10009_left.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10009_right.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10010_left.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10010_right.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10013_left.jpeg /content/dr2015-resized/manual/sample/\n",
        "!cp /content/dr2015-resized/manual/train/10013_right.jpeg /content/dr2015-resized/manual/sample/\n",
        "builder = tfds.builder(name=\"diabetic_retinopathy_detection/btgraham-300\", data_dir='dr2015-resized')\n",
        "builder.download_and_prepare(download_dir='dr2015-resized')\n",
        "!mv /content/dr2015-resized/diabetic_retinopathy_detection /content/diabetic_retinopathy_detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6M5gpICCTdI"
      },
      "source": [
        "!zip -r /content/dataset.zip /content/diabetic_retinopathy_detection\n",
        "files.download('/content/dataset.zip') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf0MWApibhLg"
      },
      "source": [
        "# Retinopathy Sample Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuFMFKX7EXZH"
      },
      "source": [
        "## Load Dataset for GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6xHjYIZnPWP"
      },
      "source": [
        "# dir = \"/content/gdrive/MyDrive/dataset\"\n",
        "# ds = tfds.load(\"diabetic_retinopathy_detection/btgraham-300\", data_dir=dir)\n",
        "# train_test = ds['train'].concatenate(ds['test'])\n",
        "# train_test = train_test.map(lambda image: tf.image.resize(tf.image.convert_image_dtype(image['image'], dtype=tf.float32), [512, 512]))\n",
        "# batch_size = 18\n",
        "# train_test = train_test.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbN-ijuNc1L_"
      },
      "source": [
        "## First GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qte0fXVLE_nY"
      },
      "source": [
        "### First GAN sample generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_hhQMlURxkQ"
      },
      "source": [
        "First GAN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QUnent8wKEN"
      },
      "source": [
        "discriminator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer((512, 512, 3)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBkc6BZ9R1fa"
      },
      "source": [
        "First GAN Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFREwyptwcPi"
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "generator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer(latent_dim),\n",
        "        tf.keras.layers.Dense(128 * 128 * 128, use_bias=False, input_shape=(100,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Reshape((128, 128, 128)),\n",
        "        tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(1, 1), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation='sigmoid'),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Z__kJTR7ck"
      },
      "source": [
        "First GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03HF3AoDRDy4"
      },
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.discriminator_optimizer = d_optimizer\n",
        "        self.generator_optimizer = g_optimizer\n",
        "\n",
        "    def train_step(self, images):\n",
        "        noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "          generated_images = self.generator(noise, training=True)\n",
        "\n",
        "          real_output = self.discriminator(images, training=True)\n",
        "          fake_output = self.discriminator(generated_images, training=True)\n",
        "\n",
        "          gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "          real_loss = self.loss_fn(tf.ones_like(real_output), real_output)\n",
        "          fake_loss = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "          disc_loss = real_loss + fake_loss\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "        return {\"d_loss\": disc_loss, \"g_loss\": gen_loss}        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0xLkI2ygwmC"
      },
      "source": [
        "Checkpoint save and restore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHxnXq6gpKY"
      },
      "source": [
        "epochs = 10  # In practice, use ~300 epochs\n",
        "\n",
        "if gan1 is None:\n",
        "  gan1 = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "  gan1.compile(\n",
        "      d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "      g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "      loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "  )\n",
        "\n",
        "  checkpoint = tf.train.Checkpoint(gan1)\n",
        "  manager = tf.train.CheckpointManager(checkpoint, '/content/gdrive/My Drive/normal_gan_ckpts', max_to_keep=2)\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/gdrive/My Drive/normal_gan_ckpts'))\n",
        "\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        " def on_epoch_end(self, epoch, logs=None):\n",
        "   # manager.save()\n",
        "   predictions = self.model.generator(tf.random.normal([1, latent_dim]), training=False)\n",
        "   fig = plt.figure(figsize=(3, 3))\n",
        "   plt.imshow(predictions[0])\n",
        "   plt.axis('off')\n",
        "   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OK1orhvg246"
      },
      "source": [
        "Plot generated image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKBJ9ExVg5sX"
      },
      "source": [
        "import plotly.express as px\n",
        "generatedImage = gan1.generator(tf.random.normal(shape=(1,latent_dim)))\n",
        "px.imshow(generatedImage[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpNldSDkCJ-"
      },
      "source": [
        "### First GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN3JZQPAg9zh"
      },
      "source": [
        "# gan1.fit(train_test, epochs=epochs, callbacks=[MyCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VJPTlwpc_0R"
      },
      "source": [
        "## Second GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN69EDKVNebQ"
      },
      "source": [
        "### Second GAN sample generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFVHtC_RhgML"
      },
      "source": [
        "Second GAN Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jivBm2dChfRC"
      },
      "source": [
        "discriminator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer((512, 512, 3)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "     \n",
        "        tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same'),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtbuvMdzhY_s"
      },
      "source": [
        "Second GAN Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PcwjEl1hjHI"
      },
      "source": [
        "latent_dim = 100\n",
        "\n",
        "generator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer(latent_dim),\n",
        "        tf.keras.layers.Dense(128 * 128 * 128, use_bias=False, input_shape=(100,)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Reshape((128, 128, 128)),\n",
        "        tf.keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        tf.keras.layers.Conv2DTranspose(3, (4, 4), strides=(1, 1), padding='same', use_bias=False, activation='sigmoid'),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcvtqqbHhqwH"
      },
      "source": [
        "Second GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgpgF8khhsEf"
      },
      "source": [
        "class GAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(GAN, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.discriminator_optimizer = d_optimizer\n",
        "        self.generator_optimizer = g_optimizer\n",
        "\n",
        "    def train_step(self, images):\n",
        "        noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "          generated_images = self.generator(noise, training=True)\n",
        "\n",
        "          real_output = self.discriminator(images, training=True)\n",
        "          fake_output = self.discriminator(generated_images, training=True)\n",
        "\n",
        "          gen_loss = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "          real_loss = self.loss_fn(tf.ones_like(real_output), real_output)\n",
        "          fake_loss = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "          disc_loss = real_loss + fake_loss\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.generator_optimizer.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
        "        self.discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
        "        return {\"d_loss\": disc_loss, \"g_loss\": gen_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6qYskBdhzlZ"
      },
      "source": [
        "Checkpoint save and restore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTTgsUVDh1ba"
      },
      "source": [
        "epochs = 10  # In practice, use ~300 epochs\n",
        "\n",
        "if gan2 is None:\n",
        "  gan2 = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "  gan2.compile(\n",
        "      d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "      g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
        "      loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "  )\n",
        "\n",
        "  checkpoint = tf.train.Checkpoint(gan2)\n",
        "  manager = tf.train.CheckpointManager(checkpoint, '/content/gdrive/My Drive/deep_aug_dims_ckpts', max_to_keep=2)\n",
        "  checkpoint.restore(tf.train.latest_checkpoint('/content/gdrive/My Drive/deep_aug_dims_ckpts'))\n",
        "\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        " def on_epoch_end(self, epoch, logs=None):\n",
        "   # manager.save()\n",
        "   predictions = self.model.generator(tf.random.normal([1, latent_dim]), training=False)\n",
        "   fig = plt.figure(figsize=(3, 3))\n",
        "   plt.imshow(predictions[0])\n",
        "   plt.axis('off')\n",
        "   plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TI7C25h440"
      },
      "source": [
        "Plot generated images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K86x9NGh6F9"
      },
      "source": [
        "import plotly.express as px\n",
        "generatedImage = gan2.generator(tf.random.normal(shape=(1,latent_dim)))\n",
        "px.imshow(generatedImage[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXCpyceok0z7"
      },
      "source": [
        "### Second GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m53DXPOh9MW"
      },
      "source": [
        "# gan2.fit(train_test, epochs=epochs, callbacks=[MyCallback()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO5ib5__Nzt8"
      },
      "source": [
        "# Features Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reZOGfDoeVmA"
      },
      "source": [
        "## First GAN Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGrTk8ZquG7"
      },
      "source": [
        "Extractor build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN2UYEeTjrdw"
      },
      "source": [
        "if gan1 is None: gan1 = restore_gan(1)\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(512, 512, 3))\n",
        "extractor = tf.keras.models.Model(gan1.discriminator.layers[0].input, gan1.discriminator.layers[-3].output)\n",
        "extractor.trainable = False\n",
        "x = extractor(input)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "extractor = tf.keras.models.Model(input, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPycuLetpHis"
      },
      "source": [
        "Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8eZoIBOaz8U"
      },
      "source": [
        "dir = \"/content/gdrive/MyDrive/dataset\"\n",
        "ds = tfds.load(\"diabetic_retinopathy_detection/btgraham-300\", data_dir=dir, shuffle_files=True)\n",
        "train = ds['train'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "test = ds['test'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "for i, sample in enumerate(train):\n",
        "  features.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "  labels.append(sample[1])\n",
        "  print('\\rTraining Dataset to list: %.0f%%'%(((i+1)/len(train))*100), end='')\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "test_feats = []\n",
        "test_labs = []\n",
        "for i, sample in enumerate(test):\n",
        "  test_feats.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "  test_labs.append(sample[1])\n",
        "  print('\\rValidation Dataset to list: %.0f%%'%(((i+1)/len(test))*100), end='')\n",
        "test_feats = np.array(test_feats)\n",
        "test_labs = np.array(test_labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqvClMbueavg"
      },
      "source": [
        "## Second GAN Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7lgnn2jo2I4"
      },
      "source": [
        "Extractor build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FchPVaovfVS3"
      },
      "source": [
        "if gan2 is None: gan2 = restore_gan(2)\n",
        "\n",
        "input = tf.keras.layers.Input(shape=(512, 512, 3))\n",
        "extractor = tf.keras.models.Model(gan2.discriminator.layers[0].input, gan2.discriminator.layers[-3].output)\n",
        "extractor.trainable = False\n",
        "x = extractor(input)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "extractor = tf.keras.models.Model(input, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4thxrnjo8Es"
      },
      "source": [
        "Features extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oahvwLfTfVS3"
      },
      "source": [
        "dir = \"/content/gdrive/MyDrive/dataset\"\n",
        "ds = tfds.load(\"diabetic_retinopathy_detection/btgraham-300\", data_dir=dir, shuffle_files=True)\n",
        "train = ds['train'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "test = ds['test'].map(lambda sample: (tf.image.resize(tf.image.convert_image_dtype(sample['image'], dtype=tf.float32), [512, 512]), tf.cast(sample['label'], tf.float32)))\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "for i, sample in enumerate(train):\n",
        "  features.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "  labels.append(sample[1])\n",
        "  print('\\rTraining Dataset to list: %.0f%%'%(((i+1)/len(train))*100), end='')\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "test_feats = []\n",
        "test_labs = []\n",
        "for i, sample in enumerate(test):\n",
        "  test_feats.append(extractor(tf.expand_dims(sample[0], axis=0))[0])\n",
        "  test_labs.append(sample[1])\n",
        "  print('\\rValidation Dataset to list: %.0f%%'%(((i+1)/len(test))*100), end='')\n",
        "test_feats = np.array(test_feats)\n",
        "test_labs = np.array(test_labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6ullEZfa91-"
      },
      "source": [
        "# Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wiDLJ2WbA-1"
      },
      "source": [
        "n_folds = 5\n",
        "if gan2 is None: gan2 = restore_gan(2)\n",
        "extractor = create_extractor(gan2.discriminator)\n",
        "features, labels = extract_features(extractor, type='train')\n",
        "skf = StratifiedKFold(n_folds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u-t26O2w-CP"
      },
      "source": [
        "class Regressor(tf.keras.Model):\n",
        "    def __init__(self, parameters):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.parameters = parameters\n",
        "        self.M = tf.Variable(tf.zeros([self.parameters]), dtype = tf.float32, trainable=True)\n",
        "\n",
        "    def call(self, val_feats):\n",
        "        return tf.tensordot(val_feats, self.M, axes=1)\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(Regressor, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train_on_batch(self, train_samples, train_labs, val_feats, val_labs, weights):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          labels_pred = tf.tensordot(train_samples, self.M, axes=1)\n",
        "          train_weights = np.zeros(len(train_labs))\n",
        "          for i, lab in enumerate(train_labs):\n",
        "            train_weights[i] = weights[int(lab)]\n",
        "          loss = tf.reduce_mean(train_weights * tf.square(train_labs - labels_pred))\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        if val_feats is not None and val_labs is not None:\n",
        "          labels_val = tf.tensordot(val_feats, self.M, axes=1)\n",
        "          val_weights = np.zeros(len(val_labs))\n",
        "          for i, lab in enumerate(val_labs):\n",
        "            val_weights[i] = weights[int(lab)]\n",
        "          val_loss = mean_squared_error(val_labs, labels_val, sample_weight=val_weights)\n",
        "          return loss.numpy(), val_loss\n",
        "\n",
        "        return loss.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiOEC8DElouL"
      },
      "source": [
        "def make_weights(labels, type):\n",
        "  weights = None\n",
        "  if type == 'class':\n",
        "    labels_count = np.zeros(5)\n",
        "    for l in labels:\n",
        "      labels_count[int(l)] += 1\n",
        "    weights = np.zeros(5)\n",
        "    for i, class_ in enumerate(weights):\n",
        "      weights[i] = tf.cast(len(labels) / (labels_count[i] * 5), tf.float32)\n",
        "  elif type == 'sample':\n",
        "    labels_count = np.zeros(5)\n",
        "    for l in labels:\n",
        "      labels_count[int(l)] += 1\n",
        "    weights = []\n",
        "    for i, label in enumerate(labels):\n",
        "      weights.insert(i, tf.cast(len(labels) / (labels_count[int(label)] * 5), tf.float32).numpy())\n",
        "  else:\n",
        "    print('Method not found')\n",
        "  return weights\n",
        "\n",
        "def train_and_evaluate_model(model_name, train_feats, train_labs, epochs, val_feats, val_labs):\n",
        "\n",
        "  if model_name == \"LR_Analytical_Optimization\":\n",
        "    weights = make_weights(train_labs, 'class')\n",
        "    start = time.time()\n",
        "    train_samples = tf.concat([train_feats, np.ones((len(train_feats), 1))], axis=-1)\n",
        "    val_samples = tf.concat([val_feats, np.ones((len(val_feats), 1))], axis=-1)\n",
        "    regressor = Regressor(parameters = extractor.output.shape[-1] + 1)\n",
        "    regressor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "    for i in range(0, epochs):\n",
        "      train_loss, val_loss = regressor.train_on_batch(train_samples, train_labs, val_samples, val_labs, weights)\n",
        "      print('\\r\\t\\tEpoch: %d || train_loss: %f - val_loss: %f'%(i+1, train_loss, val_loss), end='')\n",
        "    interval = time.time() - start\n",
        "\n",
        "  elif model_name == \"LR_SKlearn\":\n",
        "    c_weights = make_weights(train_labs, 'class')\n",
        "    weights = make_weights(train_labs, 'sample')\n",
        "    val_weights = np.zeros(len(val_labs))\n",
        "    for i, lab in enumerate(val_labs):\n",
        "      val_weights[i] = c_weights[int(lab)]\n",
        "      start = time.time()\n",
        "    reg = LinearRegression()\n",
        "    reg.fit(train_feats, train_labs, sample_weight=weights)\n",
        "    preds = reg.predict(val_feats)\n",
        "    val_loss = mean_squared_error(val_labs, preds, sample_weight=val_weights)\n",
        "    print('\\t\\tval_loss: %f'%(val_loss), end='')\n",
        "    interval = time.time() - start\n",
        "\n",
        "  elif model_name == \"LR_Keras_Dense_Layer\":\n",
        "    weights = make_weights(train_labs, 'class')\n",
        "    c_w = {0: weights[0], 1: weights[1], 2: weights[2], 3: weights[3], 4: weights[4]}\n",
        "    val_weights = np.zeros(len(val_labs))\n",
        "    for i, lab in enumerate(val_labs):\n",
        "      val_weights[i] = weights[int(lab)]\n",
        "    start = time.time()\n",
        "    input = tf.keras.layers.Input(shape=extractor.output.shape[-1])\n",
        "    output = tf.keras.layers.Dense(units=1)(input)\n",
        "    regressor = tf.keras.models.Model(input, output)\n",
        "    regressor.compile(optimizer=tf.optimizers.Adam(learning_rate=0.1), loss='mse')\n",
        "    for i in range(0, epochs):\n",
        "      train_loss = regressor.train_on_batch(train_feats, train_labs, class_weight=c_w)\n",
        "      val_loss = mean_squared_error(val_labs, regressor(val_feats), sample_weight=val_weights)\n",
        "      print('\\r\\t\\tEpoch: %d || train_loss: %f - val_loss: %f'%(i+1, train_loss, val_loss), end='')\n",
        "    interval = time.time() - start\n",
        "\n",
        "  return val_loss, interval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpiL37LLcorZ"
      },
      "source": [
        "models = [\"LR_Analytical_Optimization\", \"LR_SKlearn\", \"LR_Keras_Dense_Layer\"]\n",
        "\n",
        "validation_losses = {}\n",
        "for model_name in models:\n",
        "  print(\"Model: %s\"%(model_name))\n",
        "  partial_val_loss = []\n",
        "  m_interval = 0\n",
        "  for i, (train_index, test_index) in enumerate(skf.split(features, labels)):\n",
        "    print(\"\\tFold: %d/%d\"%(i+1, n_folds))\n",
        "    train_feats = features[train_index]\n",
        "    train_labs = labels[train_index]\n",
        "    val_feats = features[train_index]\n",
        "    val_labs = labels[train_index]\n",
        "    start = time.time()\n",
        "    val_loss, interval = train_and_evaluate_model(model_name, train_feats, train_labs, 2000, val_feats, val_labs)\n",
        "    m_interval += interval\n",
        "    partial_val_loss.append(val_loss)\n",
        "    print()\n",
        "  validation_losses[model_name] = (np.mean(partial_val_loss), m_interval/5)\n",
        "  print()\n",
        "\n",
        "validation_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egstmi55FCwl"
      },
      "source": [
        "# Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Ah-1eFOKyK"
      },
      "source": [
        "class Regressor(tf.keras.Model):\n",
        "    def __init__(self, parameters):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.parameters = parameters\n",
        "        self.M = tf.Variable(tf.zeros([self.parameters]), dtype = tf.float32, trainable=True)\n",
        "\n",
        "    def call(self, val_feats):\n",
        "        return tf.tensordot(val_feats, self.M, axes=1)\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(Regressor, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train_on_batch(self, train_samples, train_labs, val_feats, val_labs, weights):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "          labels_pred = tf.tensordot(train_samples, self.M, axes=1)\n",
        "          train_weights = np.zeros(len(train_labs))\n",
        "          for i, lab in enumerate(train_labs):\n",
        "            train_weights[i] = weights[int(lab)]\n",
        "          loss = tf.reduce_mean(train_weights * tf.square(train_labs - labels_pred))\n",
        "\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        if val_feats is not None and val_labs is not None:\n",
        "          labels_val = tf.tensordot(val_feats, self.M, axes=1)\n",
        "          val_weights = np.zeros(len(val_labs))\n",
        "          for i, lab in enumerate(val_labs):\n",
        "            val_weights[i] = weights[int(lab)]\n",
        "          val_loss = mean_squared_error(val_labs, labels_val, sample_weight=val_weights)\n",
        "          return loss.numpy(), val_loss\n",
        "\n",
        "        return loss.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjW-_NglOKyL"
      },
      "source": [
        "import copy\n",
        "\n",
        "def make_weights(labels, type):\n",
        "  weights = None\n",
        "  if type == 'class':\n",
        "    labels_count = np.zeros(5)\n",
        "    for l in labels:\n",
        "      labels_count[int(l)] += 1\n",
        "    weights = np.zeros(5)\n",
        "    for i, class_ in enumerate(weights):\n",
        "      weights[i] = tf.cast(len(labels) / (labels_count[i] * 5), tf.float32)\n",
        "  elif type == 'sample':\n",
        "    labels_count = np.zeros(5)\n",
        "    for l in labels:\n",
        "      labels_count[int(l)] += 1\n",
        "    weights = []\n",
        "    for i, label in enumerate(labels):\n",
        "      weights.insert(i, tf.cast(len(labels) / (labels_count[int(label)] * 5), tf.float32).numpy())\n",
        "  else:\n",
        "    print('Method not found')\n",
        "  return weights\n",
        "\n",
        "def train_and_evaluate_model(model_name, train_feats, train_labs, epochs, val_feats, val_labs):\n",
        "\n",
        "  if model_name == \"LR_Analytical_Optimization\":\n",
        "    evals = []\n",
        "    weights = make_weights(train_labs, 'class')\n",
        "    train_samples = tf.concat([train_feats, np.ones((len(train_feats), 1))], axis=-1)\n",
        "    val_samples = tf.concat([val_feats, np.ones((len(val_feats), 1))], axis=-1)\n",
        "    regressor = Regressor(parameters = extractor.output.shape[-1] + 1)\n",
        "    regressor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "    best_ckpts = {'Model': None, 'val_loss': None}\n",
        "    eps = []\n",
        "    for i in range(0, epochs):\n",
        "      train_loss, val_loss = regressor.train_on_batch(train_samples, train_labs, val_samples, val_labs, weights)\n",
        "      evals.append(val_loss)\n",
        "      # train_loss, _ = regressor.train_on_batch(train_samples, train_labs, weights=weights)\n",
        "      print('\\r\\t\\tEpoch: %d || train_loss: %f - val_loss: %f'%(i+1, train_loss, val_loss), end='')\n",
        "      if best_ckpts['val_loss'] is None or best_ckpts['val_loss'] > val_loss: \n",
        "        eps.append(i)\n",
        "        best_ckpts = {'Model': copy.copy(regressor), 'val_loss': val_loss}\n",
        "    print('\\r\\nSaved best: ', best_ckpts['val_loss'], end='')\n",
        "    predictions = best_ckpts['Model'](val_samples)\n",
        "    val_loss = evals\n",
        "\n",
        "  elif model_name == \"LR_SKlearn\":\n",
        "    c_weights = make_weights(train_labs, 'class')\n",
        "    weights = make_weights(train_labs, 'sample')\n",
        "    val_weights = np.zeros(len(val_labs))\n",
        "    for i, lab in enumerate(val_labs):\n",
        "      val_weights[i] = c_weights[int(lab)]\n",
        "    reg = LinearRegression()\n",
        "    reg.fit(train_feats, train_labs, sample_weight=weights)\n",
        "    predictions = reg.predict(val_feats)\n",
        "    val_loss = mean_squared_error(val_labs, predictions, sample_weight=val_weights)\n",
        "    print('\\t\\tval_loss: %f'%(val_loss), end='')\n",
        "\n",
        "  return val_loss, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBgRioHhOKyM"
      },
      "source": [
        "if gan2 is None: gan2 = restore_gan(2)\n",
        "extractor = create_extractor(gan2.discriminator)\n",
        "features, labels = extract_features(extractor, type='train')\n",
        "test_feats, test_labs = extract_features(extractor, type='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6RWeB6IOKyM"
      },
      "source": [
        "val_loss_sklearn, pred_sklearn = train_and_evaluate_model(\"LR_SKlearn\", \n",
        "                                      features, \n",
        "                                      labels, \n",
        "                                      10000, \n",
        "                                      test_feats, \n",
        "                                      test_labs)\n",
        "\n",
        "val_loss_opt, pred_optimization = train_and_evaluate_model(\"LR_Analytical_Optimization\", \n",
        "                                      features, \n",
        "                                      labels, \n",
        "                                      10000, \n",
        "                                      test_feats, \n",
        "                                      test_labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg4jHzhOkZYe"
      },
      "source": [
        "from sklearn import metrics\n",
        "import math\n",
        "\n",
        "for i, (label, pred_skl, pred_opt) in enumerate(zip(test_labs, pred_sklearn, pred_optimization)):\n",
        "  if i < 30:\n",
        "    print(\"GT Label: %d      SKLearn score: %f      Analytical Optimization score: %f\"%(label, pred_skl, pred_opt))\n",
        "  else: break\n",
        "\n",
        "print(\"\\n\\nSKLearn MAE: %f \\tAnalytical Optimization MAE: %f\"%(metrics.mean_absolute_error(test_labs, pred_sklearn), metrics.mean_absolute_error(test_labs, pred_optimization)))\n",
        "print(\"SKLearn RMSE: %f \\tAnalytical Optimization RMSE: %f\"%(math.sqrt(metrics.mean_squared_error(test_labs, pred_sklearn)), math.sqrt(metrics.mean_squared_error(test_labs, pred_optimization))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}